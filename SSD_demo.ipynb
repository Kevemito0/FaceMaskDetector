{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import random\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import Counter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "from tensorflow.keras.models import Model\n",
    "import glob \n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"C:/Users/Kevem/OneDrive/Documents/GitHub/FaceMaskDetector/Dataset\"\n",
    "annotations  = dataset_directory + \"/annotations\"\n",
    "image_directory = \"C:/Users/Kevem/OneDrive/Documents/GitHub/FaceMaskDetector/Dataset/images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "            \"file\":[],\n",
    "            \"name\":[],    \n",
    "            \"width\":[],\n",
    "            \"height\":[],\n",
    "            \"xmin\":[],\n",
    "            \"ymin\":[],   \n",
    "            \"xmax\":[],\n",
    "            \"ymax\":[],\n",
    "           }\n",
    "\n",
    "for anno in glob.glob(annotations+ \"/*.xml\"):\n",
    "    tree = ET.parse(anno)\n",
    "    \n",
    "    for elem in tree.iter():\n",
    "        if 'size' in elem.tag:\n",
    "            for attr in list(elem):\n",
    "                if 'width' in attr.tag: \n",
    "                    width = int(round(float(attr.text)))\n",
    "                if 'height' in attr.tag:\n",
    "                    height = int(round(float(attr.text)))    \n",
    "\n",
    "        if 'object' in elem.tag:\n",
    "            for attr in list(elem):\n",
    "                \n",
    "                if 'name' in attr.tag:\n",
    "                    name = attr.text                 \n",
    "                    dataset['name']+=[name]\n",
    "                    dataset['width']+=[width]\n",
    "                    dataset['height']+=[height] \n",
    "                    dataset['file'] += [os.path.basename(anno).split('.')[0]]\n",
    "                if 'bndbox' in attr.tag:\n",
    "                    for dim in list(attr):\n",
    "                        if 'xmin' in dim.tag:\n",
    "                            xmin = int(round(float(dim.text)))\n",
    "                            dataset['xmin']+=[xmin]\n",
    "                        if 'ymin' in dim.tag:\n",
    "                            ymin = int(round(float(dim.text)))\n",
    "                            dataset['ymin']+=[ymin]                                \n",
    "                        if 'xmax' in dim.tag:\n",
    "                            xmax = int(round(float(dim.text)))\n",
    "                            dataset['xmax']+=[xmax]                                \n",
    "                        if 'ymax' in dim.tag:\n",
    "                            ymax = int(round(float(dim.text)))\n",
    "                            dataset['ymax']+=[ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>name</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maksssksksss0</td>\n",
       "      <td>without_mask</td>\n",
       "      <td>512</td>\n",
       "      <td>366</td>\n",
       "      <td>79</td>\n",
       "      <td>105</td>\n",
       "      <td>109</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maksssksksss0</td>\n",
       "      <td>with_mask</td>\n",
       "      <td>512</td>\n",
       "      <td>366</td>\n",
       "      <td>185</td>\n",
       "      <td>100</td>\n",
       "      <td>226</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maksssksksss0</td>\n",
       "      <td>without_mask</td>\n",
       "      <td>512</td>\n",
       "      <td>366</td>\n",
       "      <td>325</td>\n",
       "      <td>90</td>\n",
       "      <td>360</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>maksssksksss1</td>\n",
       "      <td>with_mask</td>\n",
       "      <td>400</td>\n",
       "      <td>156</td>\n",
       "      <td>321</td>\n",
       "      <td>34</td>\n",
       "      <td>354</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maksssksksss1</td>\n",
       "      <td>with_mask</td>\n",
       "      <td>400</td>\n",
       "      <td>156</td>\n",
       "      <td>224</td>\n",
       "      <td>38</td>\n",
       "      <td>261</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file          name  width  height  xmin  ymin  xmax  ymax\n",
       "0  maksssksksss0  without_mask    512     366    79   105   109   142\n",
       "1  maksssksksss0     with_mask    512     366   185   100   226   144\n",
       "2  maksssksksss0  without_mask    512     366   325    90   360   141\n",
       "3  maksssksksss1     with_mask    400     156   321    34   354    69\n",
       "4  maksssksksss1     with_mask    400     156   224    38   261    73"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {\n",
    "    'with_mask': 0,\n",
    "    'mask_weared_incorrect': 1,\n",
    "    'without_mask': 2 \n",
    "}\n",
    "\n",
    "df['class'] = df['name'].map(name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'with_mask': 3232, 'without_mask': 717, 'mask_weared_incorrect': 123})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='count', ylabel='name'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(Counter(df['name']))\n",
    "sns.countplot(df['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mask_weared_incorrect', 'with_mask', 'without_mask'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(df.name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with 'with_mask': 3232\n",
      "Number of entries with 'mask_weared_incorrect': 123\n",
      "Number of entries with 'without_mask': 717\n"
     ]
    }
   ],
   "source": [
    "with_mask_count = df[df['name'] == 'with_mask'].shape[0]\n",
    "mask_weared_incorrect_count = df[df['name'] == 'mask_weared_incorrect'].shape[0]\n",
    "without_mask_count = df[df['name'] == 'without_mask'].shape[0]\n",
    "print(f\"Number of entries with 'with_mask': {with_mask_count}\")\n",
    "print(f\"Number of entries with 'mask_weared_incorrect': {mask_weared_incorrect_count}\")\n",
    "print(f\"Number of entries with 'without_mask': {without_mask_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853\n"
     ]
    }
   ],
   "source": [
    "filenames = [*os.listdir(image_directory)]\n",
    "print(format(len(filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Kevem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSDLite320_MobileNet_V3_Large_Weights.COCO_V1`. You can also use `weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "\n",
    "# Load pre-trained SSD model (MobileNet backbone)\n",
    "model = ssdlite320_mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "model.head.classification_head = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1280, 6, kernel_size=3, stride=1, padding=1)  # 3 classes: 'with_mask', 'without_mask', 'mask_weared_incorrect'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxes: tensor([[ 79., 105., 109., 142.],\n",
      "        [185., 100., 226., 144.],\n",
      "        [325.,  90., 360., 141.]])\n",
      "Labels: tensor([2, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define your label mapping\n",
    "label_map = {'with_mask': 0, 'without_mask': 2, 'mask_weared_incorrect': 1}\n",
    "\n",
    "def parse_annotation(annotation_path):\n",
    "    # Parse XML to extract bounding boxes and labels\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        xmin = int(obj.find('bndbox/xmin').text)\n",
    "        ymin = int(obj.find('bndbox/ymin').text)\n",
    "        xmax = int(obj.find('bndbox/xmax').text)\n",
    "        ymax = int(obj.find('bndbox/ymax').text)\n",
    "\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label_map.get(label, -1))\n",
    "    \n",
    "    boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "    return boxes, labels\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(image)\n",
    "    return image\n",
    "\n",
    "def get_data(image_folder, annotation_folder, transforms=None):\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "    data = []\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        annotation_path = os.path.join(annotation_folder, image_file.replace('.png', '.xml'))\n",
    "\n",
    "        # Load image and parse annotation\n",
    "        image = load_image(image_path)\n",
    "        boxes, labels = parse_annotation(annotation_path)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "        if transforms:\n",
    "            image = transforms(image)\n",
    "\n",
    "        data.append((image, target))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "dataset = get_data(image_directory, annotations, transform)\n",
    "\n",
    "# Example to visualize the first image and its target\n",
    "image, target = dataset[0]\n",
    "print(f\"Boxes: {target['boxes']}\")\n",
    "print(f\"Labels: {target['labels']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n",
    "    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImage(image):\n",
    "    image =  adjust_gamma(image, gamma=gamma)\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    cvNet.setInput(blob)\n",
    "    detections = cvNet.forward()\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        try:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            frame = image[startY:endY, startX:endX]\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.2:\n",
    "                im = cv2.resize(frame,(img_size,img_size))\n",
    "                im = np.array(im)/255.0\n",
    "                im = im.reshape(1,124,124,3)\n",
    "                result = model.predict(im)\n",
    "                if result>0.5:\n",
    "                    label_Y = 1\n",
    "                else:\n",
    "                    label_Y = 0\n",
    "                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "                cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.15, (36,255,12), 3)\n",
    "        except:pass\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Ensure that model receives input in the correct format (check if the model needs any preprocessing)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Compute the total loss\u001b[39;00m\n\u001b[0;32m     16\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\detection\\ssd.py:378\u001b[0m, in \u001b[0;36mSSD.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    375\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(features\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# compute the ssd heads outputs using the features\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m head_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# create the set of anchors\u001b[39;00m\n\u001b[0;32m    381\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchor_generator(images, features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\detection\\ssdlite.py:92\u001b[0m, in \u001b[0;36mSSDLiteHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_regression\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregression_head(x),\n\u001b[1;32m---> 92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     93\u001b[0m     }\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (list, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!list of [Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n"
     ]
    }
   ],
   "source": [
    "if isinstance(images, list):\n",
    "    images = torch.stack(images).to(device)  # Stack the list of images tensors into a single tensor and move to the device.\n",
    "else:\n",
    "    images = images.to(device)  # If images is already a tensor, move it to the device.\n",
    "\n",
    "# Ensure targets are in the correct format\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Move targets to the correct device\n",
    "\n",
    "# Forward pass: get the losses and predictions\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Ensure that model receives input in the correct format (check if the model needs any preprocessing)\n",
    "loss_dict = model(images, targets)\n",
    "\n",
    "# Compute the total loss\n",
    "losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "# Backpropagation\n",
    "losses.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> \n",
       "\n",
       " conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">122</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> \n",
       "\n",
       " conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> \n",
       "\n",
       " max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">460800</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">460800</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">23,040,050</span> \n",
       "\n",
       " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " conv2d (\u001b[38;5;33mConv2D\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m896\u001b[0m \n",
       "\n",
       " conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m122\u001b[0m, \u001b[38;5;34m64\u001b[0m)           \u001b[38;5;34m18,496\u001b[0m \n",
       "\n",
       " conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)          \u001b[38;5;34m73,856\u001b[0m \n",
       "\n",
       " max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " flatten (\u001b[38;5;33mFlatten\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m460800\u001b[0m)                      \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m460800\u001b[0m)                      \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                 \u001b[38;5;34m23,040,050\u001b[0m \n",
       "\n",
       " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m51\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,133,349</span> (88.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,133,349\u001b[0m (88.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,133,349</span> (88.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,133,349\u001b[0m (88.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(124,124,3)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    " \n",
    "# model.add(Flatten())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train, test = train_test_split(filenames, test_size=0.1, random_state=22)\n",
    "# test, val = train_test_split(test, test_size=0.7, random_state=22)\n",
    "# print(\"Length of Train =\",len(train))\n",
    "# print(\"=\"*30)\n",
    "# print(\"Length of Valid =\",len(val))\n",
    "# print(\"=\"*30)\n",
    "# print(\"Length of test =\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001) ,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (4072, 124, 124, 3)\n",
      "Shape of Y: (4072,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to collect images\n",
    "image_list = []\n",
    "Y = []\n",
    "for index, row in df.iterrows():\n",
    "    # Get the image file name and bounding box coordinates\n",
    "    image_file = row['file']\n",
    "    xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
    "    Y.append(row['class'])\n",
    "    \n",
    "    # Load the image\n",
    "    img = cv2.imread(f'{image_directory}/{image_file}.png')\n",
    "    \n",
    "    # Check if the image was loaded successfully\n",
    "    if img is not None:\n",
    "        # Crop the image based on the bounding box\n",
    "        cropped_img = img[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        # Resize the cropped image to the desired size (e.g., 124x124)\n",
    "        resized_img = cv2.resize(cropped_img, (124, 124))\n",
    "        \n",
    "        # Normalize the image by dividing by 255.0\n",
    "        normalized_img = resized_img / 255.0\n",
    "        \n",
    "        # Append the normalized image to the list\n",
    "        image_list.append(normalized_img)\n",
    "    else:\n",
    "        print(f\"Image {image_file} not found.\")\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "X = np.array(image_list) / 255.0\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Check the shape of X\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(X, Y,train_size=0.8,random_state=0)\n",
    "xtrain,xval,ytrain,yval=train_test_split(xtrain, ytrain,train_size=0.875,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False,    \n",
    "        rotation_range=15,    \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,  \n",
    "        horizontal_flip=True,  \n",
    "        vertical_flip=False)\n",
    "datagen.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='accuracy',patience=4,mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to prevent OOM errors\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU is configured!\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m179/179\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 280ms/step - accuracy: 0.7625 - loss: 0.6808 - val_accuracy: 0.7990 - val_loss: 0.6352\n",
      "Epoch 2/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 276ms/step - accuracy: 0.7695 - loss: 0.6263 - val_accuracy: 0.6225 - val_loss: 0.4227\n",
      "Epoch 3/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 275ms/step - accuracy: 0.6615 - loss: 0.3971 - val_accuracy: 0.6373 - val_loss: 0.2610\n",
      "Epoch 4/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 277ms/step - accuracy: 0.6430 - loss: 0.2601 - val_accuracy: 0.6324 - val_loss: 0.2076\n",
      "Epoch 5/50\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 275ms/step - accuracy: 0.6263 - loss: 0.1636 - val_accuracy: 0.6495 - val_loss: 0.1422\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(datagen.flow(xtrain, ytrain, batch_size=16),\n",
    "                    epochs=50,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1,\n",
    "                    validation_data=(xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.6947 - loss: 0.0018  \n",
      "Test Loss for Model:  0.01766052283346653\n",
      "Test Accuracy for Model:  0.7104294300079346\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_accuracy = model.evaluate(xtest, ytest)\n",
    "print('Test Loss for Model: ',test_loss)\n",
    "print('Test Accuracy for Model: ',test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevem\\AppData\\Local\\Temp\\ipykernel_44252\\391087889.py:7: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'],'g')\n",
    "plt.plot(history.history['val_accuracy'],'b')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevem\\AppData\\Local\\Temp\\ipykernel_44252\\2995523894.py:7: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.plot(history.history['loss'],'g')\n",
    "plt.plot(history.history['val_loss'],'b')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"mask_prediction.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvNet = cv2.dnn.readNetFromCaffe('model/architecture.txt','model/weights.caffemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n",
    "    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImage(image):\n",
    "    image =  adjust_gamma(image, gamma=gamma)\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    cvNet.setInput(blob)\n",
    "    detections = cvNet.forward()\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        try:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            frame = image[startY:endY, startX:endX]\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.2:\n",
    "                im = cv2.resize(frame,(img_size,img_size))\n",
    "                im = np.array(im)/255.0\n",
    "                im = im.reshape(1,124,124,3)\n",
    "                result = model.predict(im)\n",
    "                if result>0.5:\n",
    "                    label_Y = 1\n",
    "                else:\n",
    "                    label_Y = 0\n",
    "                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "                cv2.putText(image, assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.15, (36,255,12), 3)\n",
    "        except:pass\n",
    "    return image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
